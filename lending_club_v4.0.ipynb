{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices, dmatrix\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Activation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_2015 = pd.read_csv('lc_2015.csv', header=1, low_memory=False)\n",
    "lc_2015_dec = pd.read_csv('lc_2015_dec.csv', header=1, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickles\n",
    "\n",
    "file = open('prob_A_model', 'rb')\n",
    "logistic_A = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_2015['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate Jeremy's column\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "days_since_last_credit_pull_d = []\n",
    "for i in lc_2015['last_credit_pull_d']:\n",
    "    try:\n",
    "        temp_date = datetime.strptime(i, '%b-%Y')\n",
    "        days_since_last_credit_pull_d.append((datetime.today() - temp_date).days)\n",
    "    except:\n",
    "        days_since_last_credit_pull_d.append(0)\n",
    "        \n",
    "lc_2015['days_since_last_credit_pull_d'] = days_since_last_credit_pull_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lc_d_2015 = lc_2015[((lc_2015['loan_status']=='Fully Paid') | (lc_2015['loan_status']=='Charged Off'))].copy()\n",
    "\n",
    "lc_d_2015 = lc_2015[((lc_2015['grade']=='D')\n",
    "                   | (lc_2015['grade']=='E'))\n",
    "                   #| (lc_2015['grade']=='F')\n",
    "                   #| (lc_2015['grade']=='G'))\n",
    "                   & (lc_2015['term']==' 36 months')\n",
    "                   & ((lc_2015['loan_status']=='Fully Paid') | (lc_2015['loan_status']=='Charged Off'))\n",
    "                  ].copy()\n",
    "\n",
    "lc_d_2015.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_d_2015['loan_status'].value_counts()/np.sum(lc_d_2015['loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_d_2015['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the dataframe\n",
    "\n",
    "# Make my outcome variable 0s and 1s\n",
    "outcome_var = []\n",
    "for i in lc_d_2015['loan_status']:\n",
    "    if i == 'Charged Off':\n",
    "        outcome_var.append(1)\n",
    "    else:\n",
    "        outcome_var.append(0)\n",
    "lc_d_2015['outcome_var'] = outcome_var\n",
    "\n",
    "# Find all the mths_since columns\n",
    "mths_since_list = [i for i in lc_d_2015.columns if 'mths_since' in i]\n",
    "\n",
    "# Bin the mths_since columns to make them valid inputs\n",
    "for col in mths_since_list:\n",
    "    if col != 'sec_app_mths_since_last_major_derog':\n",
    "        category = list(pd.cut(lc_d_2015[col], bins=4, labels=['0', '1', '2', '3']))\n",
    "        for i, val in enumerate(category):\n",
    "            if type(val) != str:\n",
    "                if np.isnan(val):\n",
    "                    category[i] = '4'\n",
    "    lc_d_2015[col + '_cat'] = category\n",
    "    \n",
    "# Replace nulls in columns with strings (i.e. if no job title, replace null with 'none')\n",
    "ok_list = ['emp_title','emp_length']\n",
    "for col in ok_list:\n",
    "    new_list = []\n",
    "    col_values = list(lc_d_2015[col])\n",
    "    for i, val in enumerate(col_values):\n",
    "        if type(val) != str:\n",
    "            if np.isnan(val):\n",
    "                new_list.append('none')\n",
    "            else:\n",
    "                new_list.append(val)\n",
    "        else:\n",
    "            new_list.append(val)\n",
    "    new_col_name = col + '_2'\n",
    "    lc_d_2015[new_col_name] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the nulls for each column and store each count in a dictionary (keys = column name)\n",
    "def count_null(df):\n",
    "    null_dict = {}\n",
    "    for i in df.columns:\n",
    "        if i in null_dict:\n",
    "            null_dict[i] += sum([1 for j in lc_d_2015[i].isna() if j])\n",
    "        else:\n",
    "            null_dict[i] = sum([1 for j in lc_d_2015[i].isna() if j])\n",
    "    return null_dict\n",
    "\n",
    "null_dict = count_null(lc_d_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check df shape before dropping nulls\n",
    "lc_d_2015.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\n",
    "for key, val in null_dict.items():\n",
    "    if val >= 5000:\n",
    "        lc_d_2015.drop(labels=key, axis=1, inplace=True)\n",
    "        \n",
    "null_dict2 = count_null(lc_d_2015)\n",
    "\n",
    "lc_d_2015.dropna(inplace=True)\n",
    "lc_d_2015.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_d_2015['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include?        \tLoanStatNew\n",
    "#Keep for agg    \tid\n",
    "#Maybe           \tlast_pymnt_amnt\n",
    "#Maybe           \tlast_pymnt_d\n",
    "#Outcome         \tloan_status\n",
    "#Keep for agg    \tmember_id\n",
    "#Maybe           \tsub_grade\n",
    "#Category        \tzip_code\n",
    "#Like Outcome    \tdebt_settlement_flag\n",
    "store_list = ['last_pymnt_amnt','last_pymnt_d','loan_status','sub_grade','zip_code','debt_settlement_flag']\n",
    "kept_df = lc_d_2015[store_list].copy()\n",
    "\n",
    "remove_list = ['collection_recovery_fee',\n",
    "               #'grade',                  # <- decided to keep grade in\n",
    "               'initial_list_status',\n",
    "               'next_pymnt_d',\n",
    "               'policy_code',\n",
    "               'title',\n",
    "               'url',\n",
    "               'payment_plan_start_date',\n",
    "               'orig_projected_additional_accrued_interest',\n",
    "               'debt_settlement_flag_date',\n",
    "               'settlement_date']\n",
    "\n",
    "for i in remove_list:\n",
    "    if i in lc_d_2015.columns:\n",
    "        lc_d_2015.drop(labels=i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_d_2015.groupby(by=['loan_status','sub_grade']).count()['int_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_x_str = 'days_since_last_credit_pull_d'\n",
    "var_y_str = 'outcome_var'\n",
    "sns.scatterplot(x=var_x_str, y=var_y_str, data=lc_d_2015, alpha=0.2);\n",
    "print(lc_d_2015[[var_x_str, var_y_str]].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More cleaning and feature engineering\n",
    "\n",
    "convert_list = ['int_rate','revol_util']\n",
    "for col in convert_list:\n",
    "    new_list = []\n",
    "    new_name = col + '_float'\n",
    "    for i in lc_d_2015[col]:\n",
    "        if col != 'revol_util':\n",
    "            new_list.append(float(re.findall(r'\\d+\\.\\d+', i)[0])/100)\n",
    "        else:\n",
    "            new_list.append(float(re.findall(r'\\d+', i)[0])/100)\n",
    "    lc_d_2015[new_name] = new_list\n",
    "\n",
    "emp_length_int = []\n",
    "for i in lc_d_2015['emp_length_2']:\n",
    "    if i == 'none':\n",
    "        emp_length_int.append(0)\n",
    "    else:\n",
    "        emp_length_int.append(int(re.findall(r'\\d+', i)[0]))\n",
    "lc_d_2015['emp_length_int'] = emp_length_int\n",
    "\n",
    "earliest_cr_yrs = []\n",
    "for i in lc_d_2015['earliest_cr_line']:\n",
    "    earliest_cr_yrs.append(2019 - int(re.findall(r'\\d+', i)[0]))\n",
    "lc_d_2015['earliest_cr_yrs'] = earliest_cr_yrs\n",
    "\n",
    "own_home = []\n",
    "for i in lc_d_2015['home_ownership']:\n",
    "    if i == 'OWN':\n",
    "        own_home.append(1)\n",
    "    else:\n",
    "        own_home.append(0)\n",
    "lc_d_2015['own_home'] = own_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to try:\n",
    "# Use zip code to figure out if the borrower lives in urban area or not.  Also high income area or not.\n",
    "# Try to figure out how to use job title (emp_title_2)\n",
    "#    (maybe if the salary is really different from the avg salary for the job title, then it indicates lying)\n",
    "# Use earliest_cr_line to calculate the borrower's age\n",
    "\n",
    "# Figure out the features I want in my model\n",
    "model_cols = ['outcome_var',\n",
    "              #'grade',\n",
    "              #'days_since_last_credit_pull_d',  <- this variable has forward looking bias\n",
    "              'sub_grade',\n",
    "              'loan_amnt', 'int_rate_float', 'home_ownership', 'annual_inc',\n",
    "              'verification_status', 'issue_d', 'purpose', 'addr_state', \n",
    "              'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
    "              'revol_bal', 'revol_util_float', 'total_acc', 'collections_12_mths_ex_med',\n",
    "              'acc_now_delinq', 'tot_coll_amt', 'total_rev_hi_lim', 'acc_open_past_24mths', \n",
    "              'avg_cur_bal', 'bc_open_to_buy', 'chargeoff_within_12_mths', 'delinq_amnt',\n",
    "              'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n",
    "              'mths_since_recent_bc', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
    "              'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts',\n",
    "              'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
    "              'pct_tl_nvr_dlq', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', \n",
    "              'total_bc_limit', 'total_il_high_credit_limit', 'mths_since_last_delinq_cat',\n",
    "              'mths_since_last_record_cat', 'mths_since_last_major_derog_cat', 'mths_since_rcnt_il_cat',\n",
    "              'mths_since_recent_bc_cat', 'mths_since_recent_bc_dlq_cat', 'mths_since_recent_inq_cat',\n",
    "              'mths_since_recent_revol_delinq_cat', 'emp_length_int', 'earliest_cr_yrs']\n",
    "\n",
    "model_data = lc_d_2015[model_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Analysis\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_cols = ['emp_length_int', 'annual_inc', 'verification_status', 'purpose', 'home_ownership']\n",
    "\n",
    "# Add interactions\n",
    "cluster_str = cluster_cols[0]\n",
    "for i in cluster_cols[1:]:\n",
    "    cluster_str = cluster_str + ' + ' + i\n",
    "\n",
    "cluster_patsy = dmatrix(cluster_str, model_data)\n",
    "\n",
    "cluster_model = KMeans(n_clusters=5)\n",
    "cluster_model.fit(cluster_patsy)\n",
    "my_clusters = ['group_' + str(i) for i in cluster_model.labels_]\n",
    "\n",
    "model_data['my_clusters'] = my_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Patsy to create my X Matrix \n",
    "x_cols = ['my_clusters', 'loan_amnt', 'own_home', 'annual_inc',\n",
    "          'verification_status', 'issue_d', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
    "          'revol_bal', 'revol_util_float', 'total_acc', 'collections_12_mths_ex_med',\n",
    "          'acc_now_delinq', 'tot_coll_amt', 'total_rev_hi_lim', 'acc_open_past_24mths', \n",
    "          'avg_cur_bal', 'bc_open_to_buy', 'chargeoff_within_12_mths', 'delinq_amnt',\n",
    "          'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n",
    "          'mths_since_recent_bc', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
    "          'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts',\n",
    "          'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
    "          'pct_tl_nvr_dlq', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', \n",
    "          'total_bc_limit', 'total_il_high_credit_limit', 'mths_since_last_delinq_cat',\n",
    "          'mths_since_last_record_cat', 'mths_since_last_major_derog_cat',\n",
    "          'mths_since_recent_bc_cat', 'mths_since_recent_bc_dlq_cat',\n",
    "          'mths_since_recent_revol_delinq_cat', 'emp_length_int', 'earliest_cr_yrs']\n",
    "\n",
    "# Add interactions\n",
    "x_str = x_cols[0]\n",
    "for i in x_cols[1:]:\n",
    "    x_str = x_str + ' + ' + i\n",
    "x_str = x_str + ''\n",
    "\n",
    "x_patsy = dmatrix(x_str, model_data)\n",
    "print(x_patsy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_A = logistic_A.predict_proba(x_patsy)[:,1]\n",
    "model_data['prob_A'] = prob_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second patsy matrix that includes the new prob_A engineered feature (looks redundant but it is not)\n",
    "\n",
    "x_cols = ['my_clusters',\n",
    "          'prob_A',        # This is a new custom variable generated in another file\n",
    "          'sub_grade',\n",
    "          'loan_amnt', 'int_rate_float', 'home_ownership', 'annual_inc',\n",
    "          'verification_status', 'issue_d', 'purpose', 'addr_state', \n",
    "          'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
    "          'revol_bal', 'revol_util_float', 'total_acc', 'collections_12_mths_ex_med',\n",
    "          'acc_now_delinq', 'tot_coll_amt', 'total_rev_hi_lim', 'acc_open_past_24mths', \n",
    "          'avg_cur_bal', 'bc_open_to_buy', 'chargeoff_within_12_mths', 'delinq_amnt',\n",
    "          'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n",
    "          'mths_since_recent_bc', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
    "          'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts',\n",
    "          'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
    "          'pct_tl_nvr_dlq', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', \n",
    "          'total_bc_limit', 'total_il_high_credit_limit', 'mths_since_last_delinq_cat',\n",
    "          'mths_since_last_record_cat', 'mths_since_last_major_derog_cat', 'mths_since_rcnt_il_cat',\n",
    "          'mths_since_recent_bc_cat', 'mths_since_recent_bc_dlq_cat', 'mths_since_recent_inq_cat',\n",
    "          'mths_since_recent_revol_delinq_cat', 'emp_length_int', 'earliest_cr_yrs']\n",
    "\n",
    "# Add interactions\n",
    "x_str = x_cols[0]\n",
    "for i in x_cols[1:]:\n",
    "    x_str = x_str + ' + ' + i\n",
    "x_str = x_str + '+ loan_amnt*int_rate_float'\n",
    "\n",
    "x_patsy = dmatrix(x_str, model_data)\n",
    "print(x_patsy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "corrs = model_data.corr()\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(corrs, square=True, cmap=\"RdBu_r\");\n",
    "plt.savefig(fname='corr_matrix', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train test split\n",
    "\n",
    "x_raw = x_patsy\n",
    "y_raw = np.array(model_data['outcome_var'])\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n",
    "\n",
    "sss1.get_n_splits()\n",
    "for train_index, test_index in sss1.split(x_raw, y_raw):\n",
    "    x_mid, x_test = x_raw[train_index, :], x_raw[test_index, :]\n",
    "    y_mid, y_test = y_raw[train_index], y_raw[test_index]\n",
    "\n",
    "sss2.get_n_splits()\n",
    "for train_index, test_index in sss2.split(x_mid, y_mid):\n",
    "    x_train, x_val = x_mid[train_index, :], x_mid[test_index, :]\n",
    "    y_train, y_val = y_mid[train_index], y_mid[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample defaults (outcome_var=1)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "x_train_0 = x_train[y_train==0,:]\n",
    "x_train_1 = x_train[y_train==1,:]\n",
    " \n",
    "# Upsample minority class\n",
    "x_train_1_up = resample(x_train_1, replace=True, n_samples=x_train_0.shape[0], random_state=0)\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "x_train_up = np.append(x_train_0, x_train_1_up, axis=0)\n",
    "y_train_up = np.append(np.zeros(x_train_0.shape[0]), np.ones(x_train_1_up.shape[0]))\n",
    " \n",
    "print(x_train_0.shape)\n",
    "print(x_train_1_up.shape)\n",
    "print(x_train_up.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize my variables\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_up)\n",
    "x_norm_train = scaler.transform(x_train_up)\n",
    "x_norm_val = scaler.transform(x_val)\n",
    "x_norm_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (knn)\n",
    "\n",
    "# ks = [501]\n",
    "# param_grid = [{'n_neighbors': ks}]\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "# knn_grid = GridSearchCV(knn, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "# knn_grid.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (logit)\n",
    "\n",
    "penalty = ['l2']\n",
    "C = np.logspace(0, 4, 10, 100, 1000)\n",
    "param_grid = dict(C=C, penalty=penalty)\n",
    "\n",
    "logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logistic_grid = GridSearchCV(logistic, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "logistic_grid.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (SVM)\n",
    "\n",
    "# C = [1]\n",
    "# gammas = [0.001, 0.1]\n",
    "# param_grid = dict(C=C, gamma=gammas)\n",
    "\n",
    "# svm1 = svm.SVC(kernel='rbf', probability=True)\n",
    "# svm_grid = GridSearchCV(svm1, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "# svm_grid.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Naive Bayes Model\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb_best = gnb.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV with 5 folds (Random Forest)\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'n_estimators': [500, 700]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_grid = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "rf_grid.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a neural network that doesn't crash your kernel\n",
    "\n",
    "# n_cols = x_norm_val.shape[1]\n",
    "# nn_model = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(n_cols,100))\n",
    "\n",
    "# nn_model.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "\n",
    "bagging_logit = BaggingClassifier(linear_model.LogisticRegression(solver='lbfgs', max_iter=10000, C=100000), \n",
    "                                  n_estimators=1000, max_samples=0.50, max_features=0.80, verbose=10)\n",
    "bagging_logit.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "# params = {'n_estimators': 500,\n",
    "#           'max_leaf_nodes': 5, \n",
    "#           'max_depth': None, \n",
    "#           'min_samples_split': 5,\n",
    "#           'learning_rate': 0.1, \n",
    "#           'subsample': 0.20}\n",
    "\n",
    "# gboost = GradientBoostingClassifier(**params)\n",
    "# gboost.fit(x_norm_train, y_train_up)\n",
    "\n",
    "# Grid search and CV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'max_depth': [15],\n",
    "#     'min_samples_leaf': [10],\n",
    "#     'min_samples_split': [5],\n",
    "#     'n_estimators': [700],\n",
    "#     'learning_rate': [0.15], \n",
    "#     'subsample': [0.50]\n",
    "# }\n",
    "\n",
    "# gboost = GradientBoostingClassifier(max_depth=20, min_samples_leaf=10, min_samples_split=10, n_estimators=700,\n",
    "#                                     learning_rate=0.15, subsample=0.50, verbose=10)\n",
    "# #gboost_grid = GridSearchCV(gboost, param_grid, cv=5, scoring='roc_auc', verbose=10, n_jobs=-1)\n",
    "# gboost.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a neural network\n",
    "# nn_model = Sequential()\n",
    "\n",
    "# #get number of columns in training data\n",
    "# n_cols = x_norm_val.shape[1]\n",
    "\n",
    "# #add model layers\n",
    "# nn_model.add(Dense(10, activation='relu', input_dim=n_cols))\n",
    "# nn_model.add(Dense(10, activation='relu'))\n",
    "# nn_model.add(Dense(1))\n",
    "\n",
    "# # compiling the model\n",
    "# nn_model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "\n",
    "# # training the model\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping_monitor = EarlyStopping(patience=3)\n",
    "# nn_model.fit(x_norm_train, y_train_up, epochs=30, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Best ROC_AUC for knn: %0.4f' % knn_grid.best_score_)\n",
    "print('Best ROC_AUC for logit: %0.4f' % logistic_grid.best_score_)\n",
    "#print('Best ROC_AUC for svm: %0.4f' % svm_grid.best_score_)\n",
    "print('Best ROC_AUC for rf: %0.4f' % rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Best Params for knn: ', knn_grid.best_params_)\n",
    "print('Best Params for logit: ', logistic_grid.best_params_)\n",
    "#print('Best Params for svm: ', svm_grid.best_params_)\n",
    "print('Best Params for rf: ', rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sub_grade Classifier\n",
    "\n",
    "x_patsy_sg_train = x_norm_train[:,5:14]\n",
    "x_patsy_sg_val = x_norm_val[:,5:14]\n",
    "x_patsy_sg_test = x_norm_test[:,5:14]\n",
    "\n",
    "logistic_sub_grade = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logistic_sub_grade.fit(x_patsy_sg_train, y_train_up)\n",
    "logistic_sub_grade.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Dummy Classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "dummy.fit(x_norm_train, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ROC for all the models\n",
    "\n",
    "# model_list = [knn_grid.best_estimator_, \n",
    "#               logistic_grid.best_estimator_, \n",
    "#               svm_grid.best_estimator_, \n",
    "#               gnb_best, \n",
    "#               rf_grid.best_estimator_,\n",
    "#               'ensemble',\n",
    "#               logistic_sub_grade,\n",
    "#               dummy]\n",
    "# model_name = ['knn', 'logit', 'svm', 'n_bayes', 'random_forest', 'ensemble', 'sg_only', 'dummy']\n",
    "\n",
    "# ROC for just the FASTEST models\n",
    "\n",
    "# model_list = [logistic_grid.best_estimator_, \n",
    "#               gnb_best, \n",
    "#               rf_grid.best_estimator_,\n",
    "#               'ensemble',\n",
    "#               logistic_sub_grade,\n",
    "#               dummy]\n",
    "# model_name = ['logit', 'n_bayes', 'random_forest', 'ensemble', 'sg_only', 'dummy']\n",
    "\n",
    "# ROC for testing\n",
    "\n",
    "#model_list = [gboost,\n",
    "#              bagging_logit,\n",
    "#              logistic_grid.best_estimator_,\n",
    "#              rf_grid.best_estimator_,\n",
    "#              logistic_sub_grade,\n",
    "#              dummy]\n",
    "#model_name = ['gboost', 'bagging', 'logit', 'rand_forest', 'sg_only', 'dummy']\n",
    "\n",
    "model_list = [bagging_logit,\n",
    "              logistic_grid.best_estimator_,\n",
    "              rf_grid.best_estimator_,\n",
    "              gnb,\n",
    "              logistic_sub_grade]\n",
    "model_name = ['bagging', 'logit', 'rand_forest', 'naive_bayes', 'sg_only']\n",
    "\n",
    "# Plot ROC curve for all my models\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for i, model in enumerate(model_list):\n",
    "    if model == 'ensemble':\n",
    "        w1 = 0.50\n",
    "        w2 = 0.50\n",
    "        y_pred = (w1*logistic_grid.best_estimator_.predict_proba(x_norm_val)[:,1] \n",
    "                  + w2*rf_grid.best_estimator_.predict_proba(x_norm_val)[:,1]\n",
    "                  + (1-w1-w2)*gnb_best.predict_proba(x_norm_val)[:,1])\n",
    "        \n",
    "    elif model == logistic_sub_grade:\n",
    "        y_pred = list(model.predict_proba(x_patsy_sg_val)[:,1])\n",
    "    else:\n",
    "        y_pred = list(model.predict_proba(x_norm_val)[:,1])\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_val, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = (model_name[i] + ' AUC = %0.4f' % roc_auc))\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the correlation of the model errors\n",
    "\n",
    "#knn_error = y_train_up - knn_grid.predict_proba(x_norm_train)[:,1]\n",
    "logit_error = y_train_up - logistic_grid.predict_proba(x_norm_train)[:,1]\n",
    "#svm_error = y_train_up - svm_grid.predict_proba(x_norm_train)[:,1]\n",
    "gnb_error = y_train_up - gnb_best.predict_proba(x_norm_train)[:,1]\n",
    "rf_error = y_train_up - rf_grid.predict_proba(x_norm_train)[:,1]\n",
    "\n",
    "error_df = pd.DataFrame()\n",
    "#error_df['knn'] = knn_error\n",
    "error_df['logit'] = logit_error\n",
    "#error_df['svm'] = svm_error\n",
    "error_df['gnb'] = gnb_error\n",
    "error_df['rand_forest'] = rf_error\n",
    "\n",
    "error_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cutoff by minimizing default rate within the loans chosen by my model\n",
    "\n",
    "cutoff = 0.18\n",
    "y_pred = rf_grid.best_estimator_.predict_proba(x_norm_val)\n",
    "\n",
    "y_class = []\n",
    "for i in y_pred[:,1]:\n",
    "    if i >= cutoff:\n",
    "        y_class.append(1)\n",
    "    else:\n",
    "        y_class.append(0)\n",
    "    \n",
    "conf_mat = metrics.confusion_matrix(y_val, y_class)\n",
    "print(conf_mat)\n",
    "my_default_rate = conf_mat[1,0]/(conf_mat[0,0] + conf_mat[1,0])\n",
    "total_default_rate = np.sum(conf_mat, axis=1)[1]/(np.sum(conf_mat, axis=1)[0] + np.sum(conf_mat, axis=1)[1])\n",
    "print(my_default_rate, total_default_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For backtest, randomly pick baskets of 100 loans and compare their performance with \n",
    "# 100 loans picked from the loans that the model likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
